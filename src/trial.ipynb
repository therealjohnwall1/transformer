{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "sd_hf = model_hf.state_dict()\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "# 50257 x 768 (lookup table for tokens)\n",
    "# 50257 = # of tokens\n",
    "# 768 = dimension of each token\n",
    "\n",
    "#1024 x 768 (lookup table for position)\n",
    "# 1024 = max sequence length, used for where the word is placed\n",
    "# 768 = dimension of each token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='openai-community/gpt2')\n",
    "set_seed(42)\n",
    "\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to train the model -> train it off tiny shakespeare dataset \n",
    "with open('../neurals/dataset/tinyShakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "data = text[:1000]\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken \n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "buf = torch.tensor(tokens[:24 + 1])\n",
    "\n",
    "# batch our tokens \n",
    "x = buf[:-1].view(4,6)\n",
    "y = buf[1:].view(4,6)\n",
    "\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = load_dataset(\"databricks/databricks-dolly-15k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
    "\n",
    "def tokenize(doc):\n",
    "    txt = doc[\"context\"] +  doc[\"instruction\"] + doc[\"response\"]\n",
    "    tokens = [eot]\n",
    "    tokens.extend(enc.encode_ordinary(txt))\n",
    "    tokens_np = np.array(tokens)\n",
    "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "    return tokens_np_uint16\n",
    "\n",
    "nprocs = max(1, os.cpu_count()//2)\n",
    "with mp.Pool(nprocs) as pool:\n",
    "    for tokens in pool.imap(tokenize, db['train'], chunksize=15):\n",
    "        print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# breaking at shard 3\n",
    "\n",
    "\n",
    "z = np.load('dolly_dataset/dolly_train_000001.npy')\n",
    "z.shape\n",
    "\n",
    "100000/(16*1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
